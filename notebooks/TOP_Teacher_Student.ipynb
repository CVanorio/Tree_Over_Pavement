{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tree-Over-Pavement Segmentation Experiments\n",
        "\n",
        "This notebook trains and evaluates several models for detecting **tree canopy over pavement (TOP)** using high-resolution NAIP imagery.  \n",
        "\n",
        "This script uses an existing dataset folder stored in [Google Drive](https://drive.google.com/drive/folders/1IohJ4p7b2hXmYYkassmejs-5YOzG1LSP?usp=drive_link). (To use this, make a shortcut to the folder in your own google drive by right clicking the folder, select organize, the click add shortcut.)\n",
        "\n",
        "We train and compare four models:\n",
        "\n",
        "1. **Teacher UNet (6-band):**  \n",
        "   Uses RGB, NIR, and two auxiliary masks (tree + paved) to produce high-quality predictions.\n",
        "\n",
        "2. **Student UNet (NAIP-only) with Knowledge Distillation:**  \n",
        "   Uses only RGB/NIR but learns from the teacher’s predictions to recover missing context.\n",
        "\n",
        "3. **Baseline UNet (NAIP-only):**  \n",
        "   Same architecture as the student but trained without teacher knowledge distillation.\n",
        "\n",
        "The notebook also handles:\n",
        "- class-imbalance weighting\n",
        "- LR scheduling + early stopping\n",
        "- full test metrics (mIoU, F1, per-class precision/recall)\n",
        "- visual comparisons across all models\n",
        "\n",
        "This establishes a controlled framework for studying how well NAIP-only models can recover the complex TOP signal compared to a fully informed 6-band teacher.\n"
      ],
      "metadata": {
        "id": "dd44OacXzNzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup & Imports\n",
        "   Install dependencies, mount Google Drive, and import all required libraries."
      ],
      "metadata": {
        "id": "bKZt05Y3yBzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install rasterio torchinfo torchmetrics segmentation_models_pytorch scikit-learn\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchinfo import summary\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q170Sz2-lwIE",
        "outputId": "9f0d81e6-3c59-4e36-df0f-1dae4c17f25b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Global Configuration and Hyperparameters\n",
        "Here we define dataset paths, class definitions, NAIP channels, and global training hyperparameters used across teacher, student, baseline models.  \n",
        "These parameters control optimizer settings, learning rates, patience for early stopping, knowledge distillation strength, and loss weighting for class imbalance.\n"
      ],
      "metadata": {
        "id": "bEJGYzb2zK9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOBAL Variables\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Paths\n",
        "dataset_root = \"/content/drive/MyDrive/PhD/Research/Research/TOP/dataset\" # REPLACE with the path to your copy of the dataset folder your google drive or your local dataset folder created by the preprocessing scripts\n",
        "\n",
        "# Training sizes\n",
        "EPOCHS_TEACHER = 100\n",
        "EPOCHS_STUDENT = 100\n",
        "EPOCHS_BASELINE = 100\n",
        "\n",
        "# Optimizers\n",
        "TEACHER_LR = 1e-3\n",
        "STUDENT_LR = 1e-3\n",
        "BASELINE_LR = 1e-3\n",
        "\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Early stopping / LR schedule controls\n",
        "LR_MIN      = 1e-5     # minimum learning rate\n",
        "LR_PATIENCE = 3        # epochs of no val improvement at LR_MIN before stop\n",
        "IMPROVE_EPS = 1e-4     # minimal improvement threshold in mIoU\n",
        "\n",
        "# Distillation parameters\n",
        "KD_ALPHA = 0.8   # weight on CE\n",
        "KD_T     = 2.0   # temperature for distillation\n",
        "\n",
        "# Gradient clipping\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "# Channels for NAIP-only model\n",
        "NAIP_CHANNELS = (0, 1, 2, 3)\n",
        "\n",
        "# Classes\n",
        "N_CLASSES = 3\n"
      ],
      "metadata": {
        "id": "40RzEagDpasf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167565f2-bd14-4372-a502-15a48f202bd0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Augmentation and Dataset Loading\n",
        "We apply lightweight geometric augmentations (random flips and rotations) to increase model robustness.  \n",
        "Augmentation is used **only for the training dataset**, helping the UNets generalize without altering the validation or test distributions.\n"
      ],
      "metadata": {
        "id": "E08b2Bm10y-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomFlipRotate:\n",
        "    def __call__(self, img_t, mask_t):\n",
        "        # img_t: (C,H,W), mask_t: (H,W)\n",
        "\n",
        "        # horiz flip\n",
        "        if random.random() < 0.5:\n",
        "            img_t = torch.flip(img_t, dims=[2])\n",
        "            mask_t = torch.flip(mask_t, dims=[1])\n",
        "\n",
        "        # vert flip\n",
        "        if random.random() < 0.5:\n",
        "            img_t = torch.flip(img_t, dims=[1])\n",
        "            mask_t = torch.flip(mask_t, dims=[0])\n",
        "\n",
        "        # random 90 deg rotation\n",
        "        k = random.randint(0, 3)\n",
        "        if k > 0:\n",
        "            img_t = torch.rot90(img_t, k, dims=[1, 2])\n",
        "            mask_t = torch.rot90(mask_t, k, dims=[0, 1])\n",
        "\n",
        "        return img_t, mask_t\n"
      ],
      "metadata": {
        "id": "fjZ0Ur9yoVUX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Definition and Dataloaders\n",
        "This section defines the PyTorch dataset class used to load 6-band input tiles and 3-class segmentation masks.  \n",
        "We use the train/val/test splits created from the preprocessing scripts, and create DataLoaders for efficient batching during training and evaluation.\n"
      ],
      "metadata": {
        "id": "X7Na9Iav061M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NAIPSegDataset(Dataset):\n",
        "    \"\"\"\n",
        "    6-band input (R,G,B,NIR, tree_binary, paved_binary)\n",
        "    Label: single-band class map with values {0,1,2}.\n",
        "    \"\"\"\n",
        "    def __init__(self, split_root, transform=None):\n",
        "        self.img_dir = os.path.join(split_root, \"images\")\n",
        "        self.lab_dir = os.path.join(split_root, \"labels\")\n",
        "        self.img_files = sorted(glob.glob(os.path.join(self.img_dir, \"*_stack.tif\")))\n",
        "        if len(self.img_files) == 0:\n",
        "            raise RuntimeError(f\"No *_stack.tif files found in {self.img_dir}\")\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_files[idx]\n",
        "        base = os.path.basename(img_path).replace(\"_stack.tif\", \"\")\n",
        "        lab_path = os.path.join(self.lab_dir, f\"{base}_label.tif\")\n",
        "\n",
        "        # 6-band image\n",
        "        with rasterio.open(img_path) as src:\n",
        "            img = src.read().astype(np.float32)  # (C,H,W)\n",
        "\n",
        "        # normalize NAIP bands (0-3) to 0-1\n",
        "        img[0:4, :, :] = img[0:4, :, :] / 255.0\n",
        "\n",
        "        # label\n",
        "        with rasterio.open(lab_path) as src:\n",
        "            mask = src.read(1).astype(np.int64)  # (H,W)\n",
        "\n",
        "        img_t = torch.from_numpy(img)\n",
        "        mask_t = torch.from_numpy(mask)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img_t, mask_t = self.transform(img_t, mask_t)\n",
        "\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "# Create datasets & dataloaders\n",
        "train_ds = NAIPSegDataset(os.path.join(dataset_root, \"train\"), transform=RandomFlipRotate())\n",
        "val_ds   = NAIPSegDataset(os.path.join(dataset_root, \"val\"),   transform=None)\n",
        "test_ds  = NAIPSegDataset(os.path.join(dataset_root, \"test\"),  transform=None)\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "num_workers = 2\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(\"Train:\", len(train_ds), \"Val:\", len(val_ds), \"Test:\", len(test_ds))\n",
        "\n",
        "xb, yb = next(iter(train_dl))\n",
        "print(\"Batch shapes:\", xb.shape, yb.shape)  # (B,6,H,W) and (B,H,W)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNULiDzhk45f",
        "outputId": "bd6df7f3-5358-4736-ea6c-675da0390b17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1775 Val: 380 Test: 381\n",
            "Batch shapes: torch.Size([8, 6, 224, 224]) torch.Size([8, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Class Weights for Imbalanced Labels\n",
        "Due to the nature of the tree-over-pavement class, the dataset is highly imbalanced, meaning there are far less pixels belonging to that class.  \n",
        "To mitigate this, we compute class weights and then slightly boost the TOP class’s weight in the model, since it is the target class.  \n",
        "These weights are used by all UNet models during training.\n"
      ],
      "metadata": {
        "id": "Y4ku31oe1ZAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_weights(dataset, num_classes=N_CLASSES):\n",
        "    \"\"\"\n",
        "    Compute inverse-frequency class weights from a dataset.\n",
        "    Assumes labels are in {0, 1, 2}.\n",
        "    \"\"\"\n",
        "    counts = torch.zeros(num_classes, dtype=torch.float64)\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        _, mask = dataset[i]          # mask: (H,W) tensor\n",
        "        mask = mask.reshape(-1)\n",
        "\n",
        "        for c in range(num_classes):\n",
        "            counts[c] += (mask == c).sum().item()\n",
        "\n",
        "    total = counts.sum()\n",
        "\n",
        "    # inverse-frequency weights\n",
        "    weights = total / (counts + 1e-6)\n",
        "\n",
        "    # normalize so mean weight ~ 1.0\n",
        "    weights = weights / weights.mean()\n",
        "\n",
        "    print(\"Class pixel counts:\", counts.tolist())\n",
        "    print(\"Class weights (normalized):\", weights.tolist())\n",
        "    return weights.float()\n",
        "\n",
        "w = compute_class_weights(train_ds, N_CLASSES)  # [w_bg, w_top, w_tnp]\n",
        "# Bias toward TOP class\n",
        "w[0] = w[0] * 0.7    # BG\n",
        "w[1] = w[1] * 1.6    # TOP\n",
        "w[2] = w[2] * 1.1    # TNP (less boost than TOP)\n",
        "\n",
        "w = w / w.mean()\n",
        "CLASS_WEIGHTS = w.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-5w5AbpbZzA",
        "outputId": "c9efa519-b884-4da6-e7e5-63836a2ac43e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class pixel counts: [70264054.0, 15329107.0, 3469239.0]\n",
            "Class weights (normalized): [0.11611187596120413, 0.5322222046319419, 2.351665919406854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Core Metrics - Confusion Matrix, IoU, and Accuracy\n",
        "Segmentation performance is measured via a confusion matrix over all classes.  \n",
        "From this we compute per-class precision, recall, F1, IoU, as well as mean IoU and overall accuracy.  \n",
        "These metrics are used throughout training and for final test-set evaluation.\n"
      ],
      "metadata": {
        "id": "UCQ7JB_L1zqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_confusion_matrix(cm, preds, targets, num_classes):\n",
        "    \"\"\"\n",
        "    cm: (C,C) numpy or torch on CPU\n",
        "    preds, targets: (H,W) or (N,) integer tensors\n",
        "    \"\"\"\n",
        "    preds = preds.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "    k = (targets >= 0) & (targets < num_classes)\n",
        "    inds = num_classes * targets[k] + preds[k]\n",
        "    cm_flat = cm.view(-1)\n",
        "    bincount = torch.bincount(inds, minlength=num_classes**2).to(cm_flat.dtype)\n",
        "    cm_flat += bincount\n",
        "    return cm\n",
        "\n",
        "def iou_from_confusion(cm):\n",
        "    \"\"\"\n",
        "    cm: (C,C)\n",
        "    returns per-class IoU and mean IoU\n",
        "    \"\"\"\n",
        "    cm = cm.float()\n",
        "    tp = torch.diag(cm)\n",
        "    fp = cm.sum(dim=0) - tp\n",
        "    fn = cm.sum(dim=1) - tp\n",
        "    denom = tp + fp + fn + 1e-6\n",
        "    iou = tp / denom\n",
        "    miou = iou.mean()\n",
        "    return iou, miou\n",
        "\n",
        "def accuracy_from_confusion(cm):\n",
        "    cm = cm.float()\n",
        "    correct = torch.diag(cm).sum()\n",
        "    total = cm.sum()\n",
        "    return correct / (total + 1e-6)\n"
      ],
      "metadata": {
        "id": "Rn8m41rVk_l-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Training"
      ],
      "metadata": {
        "id": "P76UR5a37Hvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teacher UNet Training (6 Band Input)\n",
        "The teacher model receives all 6 input bands (RGB, NIR, tree mask, pavement mask).  \n",
        "It is trained with weighted cross-entropy and acts as a supervision signal for the NAIP-only student model.  \n",
        "We use ReduceLROnPlateau and an early-stopping rule that stops training once the learning rate has fully decayed and validation mIoU no longer improves.\n"
      ],
      "metadata": {
        "id": "e9sEcQla2Gsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch_6band(model, dataloader, optimizer=None):\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)\n",
        "    cm = torch.zeros((N_CLASSES, N_CLASSES), dtype=torch.int64, device=\"cpu\")\n",
        "\n",
        "    total_loss = 0.0\n",
        "    batches = 0\n",
        "\n",
        "    for xb, yb in dataloader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device).long()\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model(xb)\n",
        "        loss = loss_fn(logits, yb)\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(logits.detach(), dim=1).cpu()\n",
        "        targets = yb.detach().cpu()\n",
        "        cm = update_confusion_matrix(cm, preds, targets, num_classes=N_CLASSES)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batches += 1\n",
        "\n",
        "    avg_loss = total_loss / max(batches, 1)\n",
        "    per_class_iou, miou = iou_from_confusion(cm)\n",
        "    acc = accuracy_from_confusion(cm)\n",
        "\n",
        "    return avg_loss, acc.item(), miou.item(), per_class_iou.cpu().numpy()\n",
        "\n",
        "\n",
        "def train_teacher(model, train_dl, val_dl):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=TEACHER_LR, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"max\",\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=LR_MIN\n",
        "    )\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_miou\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_miou\": [],\n",
        "        \"lr\": []\n",
        "    }\n",
        "\n",
        "    best_val_miou = -1.0\n",
        "    best_state = None\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS_TEACHER + 1):\n",
        "        train_loss, train_acc, train_miou, _ = run_epoch_6band(model, train_dl, optimizer)\n",
        "        val_loss, val_acc, val_miou, _ = run_epoch_6band(model, val_dl, optimizer=None)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_miou\"].append(train_miou)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_miou\"].append(val_miou)\n",
        "\n",
        "        # Step LR scheduler on validation performance\n",
        "        scheduler.step(val_miou)\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "        # Check improvement\n",
        "        if val_miou > best_val_miou + IMPROVE_EPS:\n",
        "            best_val_miou = val_miou\n",
        "            best_state = model.state_dict()\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        print(\n",
        "            f\"[Teacher] Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={train_loss:.4f}, train_mIoU={train_miou:.3f}, \"\n",
        "            f\"val_loss={val_loss:.4f}, val_mIoU={val_miou:.3f}, \"\n",
        "            f\"lr={current_lr:.2e}\"\n",
        "        )\n",
        "\n",
        "        # Early stopping: LR hit minimum & no improvement for LR_PATIENCE epochs\n",
        "        if current_lr <= LR_MIN + 1e-12 and no_improve_epochs >= LR_PATIENCE:\n",
        "            print(\"Early stopping (Teacher): LR at min and val mIoU plateaued.\")\n",
        "            break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model, history, best_val_miou\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c2NB0C-9lA1W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NAIP-Only Epoch Logic\n",
        "This function defines a shared epoch loop for models that only use the 4 NAIP bands (RGB + NIR).  \n",
        "The same structure is reused for both the KD-trained student model and the NAIP-only baseline.\n"
      ],
      "metadata": {
        "id": "viGQvyTp2lPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch_naip_only(model, dataloader, optimizer=None):\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)\n",
        "    cm = torch.zeros((N_CLASSES, N_CLASSES), dtype=torch.int64, device=\"cpu\")\n",
        "\n",
        "    total_loss = 0.0\n",
        "    batches = 0\n",
        "\n",
        "    for xb, yb in dataloader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device).long()\n",
        "\n",
        "        xb_naip = xb[:, NAIP_CHANNELS, :, :]\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model(xb_naip)\n",
        "        loss = loss_fn(logits, yb)\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(logits.detach(), dim=1).cpu()\n",
        "        targets = yb.detach().cpu()\n",
        "        cm = update_confusion_matrix(cm, preds, targets, num_classes=N_CLASSES)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batches += 1\n",
        "\n",
        "    avg_loss = total_loss / max(batches, 1)\n",
        "    per_class_iou, miou = iou_from_confusion(cm)\n",
        "    acc = accuracy_from_confusion(cm)\n",
        "\n",
        "    return avg_loss, acc.item(), miou.item(), per_class_iou.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "b6ArztuClBaO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student UNet Training with Knowledge Distillation\n",
        "The student receives only NAIP bands, so we guide it using results from the frozen teacher.  \n",
        "Training combines:\n",
        "- Weighted cross-entropy on ground-truth labels, and  \n",
        "- KL-divergence to the teacher’s stats (temperature T, weight α).  \n",
        "\n",
        "This helps compensate for weaker inputs by transferring structure learned by the teacher.\n"
      ],
      "metadata": {
        "id": "FInDYJgl2wrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_student_with_distillation(student, teacher, train_dl, val_dl):\n",
        "    # freeze teacher\n",
        "    teacher.eval()\n",
        "    for p in teacher.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.AdamW(student.parameters(), lr=STUDENT_LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    ce_loss_fn = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)\n",
        "    kl_loss_fn = nn.KLDivLoss(reduction=\"mean\")\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"max\",\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=LR_MIN\n",
        "    )\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_miou\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_miou\": [],\n",
        "        \"lr\": []\n",
        "    }\n",
        "\n",
        "    best_val_miou = -1.0\n",
        "    best_state = None\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS_STUDENT + 1):\n",
        "        student.train()\n",
        "        total_train_loss = 0.0\n",
        "        batches = 0\n",
        "        cm_train = torch.zeros((N_CLASSES, N_CLASSES), dtype=torch.int64)\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device).long()\n",
        "\n",
        "            xb_full = xb\n",
        "            xb_naip = xb[:, NAIP_CHANNELS, :, :]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(xb_full)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            student_logits = student(xb_naip)\n",
        "\n",
        "            loss_ce = ce_loss_fn(student_logits, yb)\n",
        "\n",
        "            p_t = F.softmax(teacher_logits / KD_T, dim=1)\n",
        "            log_p_s = F.log_softmax(student_logits / KD_T, dim=1)\n",
        "            loss_kd = kl_loss_fn(log_p_s, p_t)\n",
        "\n",
        "            loss = KD_ALPHA * loss_ce + (1 - KD_ALPHA) * loss_kd\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = torch.argmax(student_logits.detach(), dim=1).cpu()\n",
        "            targets = yb.detach().cpu()\n",
        "            cm_train = update_confusion_matrix(cm_train, preds, targets, N_CLASSES)\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            batches += 1\n",
        "\n",
        "        train_loss = total_train_loss / max(batches, 1)\n",
        "        _, train_miou = iou_from_confusion(cm_train)\n",
        "\n",
        "        val_loss, val_acc, val_miou, _ = run_epoch_naip_only(student, val_dl, optimizer=None)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_miou\"].append(train_miou.item())\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_miou\"].append(val_miou)\n",
        "\n",
        "        scheduler.step(val_miou)\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "        if val_miou > best_val_miou + IMPROVE_EPS:\n",
        "            best_val_miou = val_miou\n",
        "            best_state = student.state_dict()\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        print(\n",
        "            f\"[Student] Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={train_loss:.3f}, train_mIoU={train_miou:.3f}, \"\n",
        "            f\"val_loss={val_loss:.3f}, val_mIoU={val_miou:.3f}, \"\n",
        "            f\"lr={current_lr:.2e}\"\n",
        "        )\n",
        "\n",
        "        if current_lr <= LR_MIN + 1e-12 and no_improve_epochs >= LR_PATIENCE:\n",
        "            print(\"Early stopping (Student): LR at min and val mIoU plateaued.\")\n",
        "            break\n",
        "\n",
        "    if best_state is not None:\n",
        "        student.load_state_dict(best_state)\n",
        "\n",
        "    return student, history, best_val_miou\n"
      ],
      "metadata": {
        "id": "PDGFfqKWlEW-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Teacher and Student Models\n",
        "Here we instantiate the teacher (6-band) and student (4-band) UNets, then train them using the previously defined loops.  \n",
        "The teacher is trained first and remains frozen while supervising the student.  \n",
        "Both use the same architecture (ResNet-34 backbone) for a fair comparison."
      ],
      "metadata": {
        "id": "Mozuw3FZ3nRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_unet_6ch = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=None,\n",
        "    in_channels=6,\n",
        "    classes=N_CLASSES\n",
        ").to(device)\n",
        "\n",
        "teacher_unet_6ch, teacher_hist, teacher_best = train_teacher(\n",
        "    teacher_unet_6ch, train_dl, val_dl\n",
        ")"
      ],
      "metadata": {
        "id": "g1F4zLIMlGFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student: NAIP-only UNet (4 bands)\n",
        "student_unet_naip = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=None,\n",
        "    in_channels=len(NAIP_CHANNELS),  # 4 bands\n",
        "    classes=N_CLASSES,\n",
        ").to(device)\n",
        "\n",
        "student_unet_naip, student_hist, student_best = train_student_with_distillation(\n",
        "    student_unet_naip, teacher_unet_6ch, train_dl, val_dl\n",
        ")\n"
      ],
      "metadata": {
        "id": "HHRaizjRRIFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline UNet (NAIP-Only, No Distillation)\n",
        "\n",
        "To isolate the benefit of distillation, we train a NAIP-only UNet with the same architecture as the student but **without** any teacher supervision.  \n"
      ],
      "metadata": {
        "id": "miWJXkwm8OKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_naip_baseline(model, train_dl, val_dl):\n",
        "    \"\"\"\n",
        "    Baseline UNet, NAIP-only input (4 bands), no teacher / no KD.\n",
        "    Uses: BASELINE_LR, WEIGHT_DECAY, EPOCHS_BASELINE, GRAD_CLIP, NAIP_CHANNELS.\n",
        "    \"\"\"\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=BASELINE_LR,\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    ce_loss_fn = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"max\",\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=LR_MIN\n",
        "    )\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_miou\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_miou\": [],\n",
        "        \"lr\": []        # <<< NEW\n",
        "    }\n",
        "\n",
        "    best_val_miou = -1.0\n",
        "    best_state = None\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS_BASELINE + 1):\n",
        "        # ---- TRAIN ----\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        batches = 0\n",
        "        cm_train = torch.zeros((N_CLASSES, N_CLASSES), dtype=torch.int64)\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device).long()\n",
        "\n",
        "            xb_naip = xb[:, NAIP_CHANNELS, :, :]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(xb_naip)\n",
        "            loss = ce_loss_fn(logits, yb)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "\n",
        "            preds   = torch.argmax(logits.detach(), dim=1).cpu()\n",
        "            targets = yb.detach().cpu()\n",
        "            cm_train = update_confusion_matrix(cm_train, preds, targets, N_CLASSES)\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            batches += 1\n",
        "\n",
        "        train_loss = total_train_loss / max(batches, 1)\n",
        "        _, train_miou = iou_from_confusion(cm_train)\n",
        "\n",
        "        # ---- VALIDATION ----\n",
        "        val_loss, val_acc, val_miou, _ = run_epoch_naip_only(model, val_dl, optimizer=None)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_miou\"].append(train_miou.item())\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_miou\"].append(val_miou)\n",
        "\n",
        "        scheduler.step(val_miou)\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "        if val_miou > best_val_miou + IMPROVE_EPS:\n",
        "            best_val_miou = val_miou\n",
        "            best_state = model.state_dict()\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        print(\n",
        "            f\"[Baseline NAIP] Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={train_loss:.3f}, train_mIoU={train_miou:.3f}, \"\n",
        "            f\"val_loss={val_loss:.3f}, val_mIoU={val_miou:.3f}, \"\n",
        "            f\"lr={current_lr:.2e}\"\n",
        "        )\n",
        "\n",
        "        if current_lr <= LR_MIN + 1e-12 and no_improve_epochs >= LR_PATIENCE:\n",
        "            print(\"Early stopping (Baseline): LR at min and val mIoU plateaued.\")\n",
        "            break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model, history, best_val_miou\n"
      ],
      "metadata": {
        "id": "ObTSmMDe1i4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BASELINE: NAIP-only UNet model\n",
        "\n",
        "baseline_unet_naip = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=None,          # NO pretrained weights\n",
        "    in_channels=len(NAIP_CHANNELS),# 4 NAIP bands\n",
        "    classes=N_CLASSES\n",
        ").to(device)\n",
        "\n",
        "print(\"Baseline UNet initialized.\")\n",
        "\n",
        "\n",
        "baseline_unet_naip, baseline_hist, baseline_best_val_miou = train_naip_baseline(\n",
        "    baseline_unet_naip,\n",
        "    train_dl,\n",
        "    val_dl\n",
        ")\n",
        "\n",
        "print(\"Best baseline val mIoU:\", baseline_best_val_miou)\n"
      ],
      "metadata": {
        "id": "2jlleou01llE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training and Model Results"
      ],
      "metadata": {
        "id": "Kbfyce167f9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Diagnostics: Curves for Loss, mIoU, and Learning Rate\n",
        "\n",
        "To understand model behavior over time, we define a helper to plot:\n",
        "\n",
        "- Training and validation loss per epoch\n",
        "- Training and validation mIoU per epoch\n",
        "- Learning rate schedule per epoch  \n",
        "\n",
        "We then apply this to the teacher, student, and baseline histories to inspect convergence and over/under-fitting.\n"
      ],
      "metadata": {
        "id": "19eARWjf8bFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curves(name, history):\n",
        "    \"\"\"\n",
        "    Plots train/val loss, train/val mIoU, and LR vs epoch.\n",
        "    Expects keys: 'train_loss', 'val_loss', 'train_miou', 'val_miou', 'lr'.\n",
        "    \"\"\"\n",
        "    epochs = np.arange(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    # Loss + mIoU\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(epochs, history[\"train_loss\"], label=\"Train loss\")\n",
        "    axes[0].plot(epochs, history[\"val_loss\"], label=\"Val loss\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "    axes[0].set_title(f\"{name} – Loss\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # mIoU\n",
        "    axes[1].plot(epochs, history[\"train_miou\"], label=\"Train mIoU\")\n",
        "    axes[1].plot(epochs, history[\"val_miou\"], label=\"Val mIoU\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].set_ylabel(\"mIoU\")\n",
        "    axes[1].set_title(f\"{name} – mIoU\")\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Learning rate\n",
        "    if \"lr\" in history:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.plot(epochs, history[\"lr\"])\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Learning rate\")\n",
        "        plt.title(f\"{name} – LR schedule\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# --- Call for each model ---\n",
        "plot_training_curves(\"Teacher UNet (6-band)\", teacher_hist)\n",
        "plot_training_curves(\"Student UNet (KD, NAIP-only)\", student_hist)\n",
        "plot_training_curves(\"Baseline UNet (NAIP-only)\", baseline_hist)\n"
      ],
      "metadata": {
        "id": "QZd7Kg8Sc0MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics from the Confusion Matrix\n",
        "\n",
        "Beyond basic IoU and accuracy, we compute richer metrics from the confusion matrix:\n",
        "\n",
        "- Per-class precision, recall, and F1  \n",
        "- Per-class IoU  \n",
        "- Overall accuracy, mean F1, and mean IoU  \n",
        "\n",
        "These provide a more detailed view of how each model performs on background, TOP, and TNP classes.\n"
      ],
      "metadata": {
        "id": "JS-fjIUw82CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_from_confusion(cm):\n",
        "    \"\"\"\n",
        "    cm: (C,C) confusion matrix\n",
        "    Returns:\n",
        "        precision_per_class\n",
        "        recall_per_class\n",
        "        f1_per_class\n",
        "        iou_per_class\n",
        "        overall_accuracy\n",
        "        mean_f1\n",
        "        mean_iou\n",
        "    \"\"\"\n",
        "    cm = cm.float()\n",
        "    tp = torch.diag(cm)                     # (C,)\n",
        "    fp = cm.sum(dim=0) - tp                 # predicted positive but wrong\n",
        "    fn = cm.sum(dim=1) - tp                 # missed positive\n",
        "    tn = cm.sum() - (tp + fp + fn)\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall    = tp / (tp + fn + 1e-6)\n",
        "    f1        = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "\n",
        "    # IoU per class\n",
        "    iou = tp / (tp + fp + fn + 1e-6)\n",
        "\n",
        "    # mean metrics\n",
        "    mean_f1  = f1.mean()\n",
        "    mean_iou = iou.mean()\n",
        "\n",
        "    # overall accuracy\n",
        "    acc = (tp.sum() / (cm.sum() + 1e-6))\n",
        "\n",
        "    return (\n",
        "        precision.cpu().numpy(),\n",
        "        recall.cpu().numpy(),\n",
        "        f1.cpu().numpy(),\n",
        "        iou.cpu().numpy(),\n",
        "        acc.item(),\n",
        "        mean_f1.item(),\n",
        "        mean_iou.item()\n",
        "    )\n"
      ],
      "metadata": {
        "id": "jsDEwwE57A1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation Helpers\n",
        "\n",
        "Here we define two evaluation functions:\n",
        "\n",
        "- `eval_teacher_6band_allmetrics` for the 6-band teacher, and  \n",
        "- `eval_naip_4band_allmetrics` for the NAIP-only models.\n",
        "\n",
        "Each runs through the test set, generates a confusion matrix, and computes a cross-entropy loss.\n"
      ],
      "metadata": {
        "id": "2mQYR3Cb8__P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- EVAL FUNCTIONS ----------\n",
        "\n",
        "def eval_teacher_6band_allmetrics(model, data_dl):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    batches = 0\n",
        "    cm = torch.zeros((N_CLASSES, N_CLASSES), dtype=torch.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in data_dl:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device).long()\n",
        "\n",
        "            logits = model(xb)\n",
        "            loss   = ce(logits, yb)\n",
        "\n",
        "            preds   = torch.argmax(logits, dim=1).cpu()\n",
        "            targets = yb.cpu()\n",
        "            cm      = update_confusion_matrix(cm, preds, targets, N_CLASSES)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batches += 1\n",
        "\n",
        "    avg_loss = total_loss / max(batches, 1)\n",
        "\n",
        "    (precision, recall, f1, iou, acc, mean_f1, mean_iou) = metrics_from_confusion(cm)\n",
        "\n",
        "    return avg_loss, acc, mean_iou, mean_f1, precision, recall, f1, iou\n",
        "\n",
        "\n",
        "def eval_naip_4band_allmetrics(model, data_dl):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    batches = 0\n",
        "    cm = torch.zeros((N_CLASSES, N_CLASSES), dtype=torch.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in data_dl:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device).long()\n",
        "\n",
        "            xb_naip = xb[:, NAIP_CHANNELS, :, :]\n",
        "\n",
        "            logits = model(xb_naip)\n",
        "            loss   = ce(logits, yb)\n",
        "\n",
        "            preds   = torch.argmax(logits, dim=1).cpu()\n",
        "            targets = yb.cpu()\n",
        "            cm      = update_confusion_matrix(cm, preds, targets, N_CLASSES)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batches += 1\n",
        "\n",
        "    avg_loss = total_loss / max(batches, 1)\n",
        "    (precision, recall, f1, iou, acc, mean_f1, mean_iou) = metrics_from_confusion(cm)\n",
        "\n",
        "    return avg_loss, acc, mean_iou, mean_f1, precision, recall, f1, iou\n",
        "\n",
        "# ---------- RUN EVAL ON TEST SET ----------\n",
        "\n",
        "# Teacher\n",
        "t_loss, t_acc, t_miou, t_mf1, t_prec, t_rec, t_f1, t_iou = eval_teacher_6band_allmetrics(\n",
        "    teacher_unet_6ch, test_dl\n",
        ")\n",
        "\n",
        "# Student (KD)\n",
        "s_loss, s_acc, s_miou, s_mf1, s_prec, s_rec, s_f1, s_iou = eval_naip_4band_allmetrics(\n",
        "    student_unet_naip, test_dl\n",
        ")\n",
        "\n",
        "# Baseline\n",
        "b_loss, b_acc, b_miou, b_mf1, b_prec, b_rec, b_f1, b_iou = eval_naip_4band_allmetrics(\n",
        "    baseline_unet_naip, test_dl\n",
        ")\n",
        "\n",
        "\n",
        "# ---------- BUILD TABLE ----------\n",
        "\n",
        "rows = [\n",
        "    {\n",
        "        \"Model\": \"Teacher UNet (6-band)\",\n",
        "        \"Bands\": \"6\",\n",
        "        \"Test Loss\": t_loss,\n",
        "        \"Accuracy\": t_acc,\n",
        "        \"mIoU\": t_miou,\n",
        "        \"mF1\": t_mf1,\n",
        "        \"Precision_BG\": t_prec[0],\n",
        "        \"Recall_BG\": t_rec[0],\n",
        "        \"F1_BG\": t_f1[0],\n",
        "        \"Precision_TOP\": t_prec[1],\n",
        "        \"Recall_TOP\": t_rec[1],\n",
        "        \"F1_TOP\": t_f1[1],\n",
        "        \"Precision_TNP\": t_prec[2],\n",
        "        \"Recall_TNP\": t_rec[2],\n",
        "        \"F1_TNP\": t_f1[2],\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"Student UNet (KD, NAIP-only)\",\n",
        "        \"Bands\": \"4\",\n",
        "        \"Test Loss\": s_loss,\n",
        "        \"Accuracy\": s_acc,\n",
        "        \"mIoU\": s_miou,\n",
        "        \"mF1\": s_mf1,\n",
        "        \"Precision_BG\": s_prec[0],\n",
        "        \"Recall_BG\": s_rec[0],\n",
        "        \"F1_BG\": s_f1[0],\n",
        "        \"Precision_TOP\": s_prec[1],\n",
        "        \"Recall_TOP\": s_rec[1],\n",
        "        \"F1_TOP\": s_f1[1],\n",
        "        \"Precision_TNP\": s_prec[2],\n",
        "        \"Recall_TNP\": s_rec[2],\n",
        "        \"F1_TNP\": s_f1[2],\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"Baseline UNet (NAIP-only)\",\n",
        "        \"Bands\": \"4\",\n",
        "        \"Test Loss\": b_loss,\n",
        "        \"Accuracy\": b_acc,\n",
        "        \"mIoU\": b_miou,\n",
        "        \"mF1\": b_mf1,\n",
        "        \"Precision_BG\": b_prec[0],\n",
        "        \"Recall_BG\": b_rec[0],\n",
        "        \"F1_BG\": b_f1[0],\n",
        "        \"Precision_TOP\": b_prec[1],\n",
        "        \"Recall_TOP\": b_rec[1],\n",
        "        \"F1_TOP\": b_f1[1],\n",
        "        \"Precision_TNP\": b_prec[2],\n",
        "        \"Recall_TNP\": b_rec[2],\n",
        "        \"F1_TNP\": b_f1[2],\n",
        "    },\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df_rounded = df.round(3)\n",
        "df_rounded\n",
        "\n"
      ],
      "metadata": {
        "id": "ga45fRI04kwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative Comparison: Student vs Baseline\n",
        "\n",
        "Finally, we visualize a few random test tiles to compare:\n",
        "\n",
        "- NAIP RGB input  \n",
        "- Ground-truth labels  \n",
        "- Student (KD NAIP-only) predictions  \n",
        "- Baseline (NAIP-only) predictions  \n",
        "\n",
        "These side-by-side views highlight where knowledge distillation helps, where both models struggle, and how their errors differ in practice.\n"
      ],
      "metadata": {
        "id": "GSzRpmp39UQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colormap: 0 = background (black), 1 = TOP (yellow), 2 = TNP (green)\n",
        "label_colors = [\"black\", \"yellow\", \"green\"]\n",
        "label_cmap = ListedColormap(label_colors)\n",
        "label_norm = BoundaryNorm([-0.5, 0.5, 1.5, 2.5], label_cmap.N)\n",
        "\n",
        "def unnormalize_rgb_from_6band(img_tensor, rgb_indices=(0,1,2)):\n",
        "    \"\"\"\n",
        "    img_tensor: (6,H,W), where NAIP bands (0-3) are scaled 0-1.\n",
        "    We'll turn them back into 0-255 for display.\n",
        "    \"\"\"\n",
        "    img_np = img_tensor.cpu().numpy()\n",
        "    rgb = img_np[list(rgb_indices), :, :]          # (3,H,W), 0-1\n",
        "    rgb = np.clip(rgb * 255.0, 0, 255).astype(np.uint8)\n",
        "    rgb = np.transpose(rgb, (1,2,0))              # (H,W,3)\n",
        "    return rgb"
      ],
      "metadata": {
        "id": "6KtBf5u5lJIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_student_baseline(\n",
        "    student_model,\n",
        "    baseline_model,\n",
        "    dataset,\n",
        "    n_samples=5,\n",
        "    rgb_indices=(0,1,2),\n",
        "    naip_channels=NAIP_CHANNELS,\n",
        "    title=\"Student vs Baseline\"\n",
        "):\n",
        "    \"\"\"\n",
        "    For n_samples random tiles from `dataset`, show per row:\n",
        "      [ NAIP RGB | Label | Student pred | Baseline pred ]\n",
        "    \"\"\"\n",
        "    student_model.eval()\n",
        "    baseline_model.eval()\n",
        "\n",
        "    n_samples = min(n_samples, len(dataset))\n",
        "    idxs = np.random.choice(len(dataset), size=n_samples, replace=False)\n",
        "\n",
        "    fig, axes = plt.subplots(n_samples, 5, figsize=(20, 4 * n_samples))\n",
        "    if n_samples == 1:\n",
        "        axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for row, idx in enumerate(idxs):\n",
        "            img_t, mask_t = dataset[idx]  # img_t: (6,H,W), mask_t: (H,W)\n",
        "\n",
        "            # NAIP-only tensor for the CNNs\n",
        "            inp_naip = img_t[naip_channels, :, :].unsqueeze(0).to(device)  # (1,4,H,W)\n",
        "\n",
        "            # Student prediction\n",
        "            logits_student = student_model(inp_naip)\n",
        "            pred_student = torch.argmax(logits_student, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "            # Baseline prediction\n",
        "            logits_baseline = baseline_model(inp_naip)\n",
        "            pred_baseline = torch.argmax(logits_baseline, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "            # RGB + label\n",
        "            rgb_img = unnormalize_rgb_from_6band(img_t, rgb_indices=rgb_indices)\n",
        "            mask_np = mask_t.cpu().numpy()\n",
        "\n",
        "            ax0, ax1, ax2, ax3, ax4 = axes[row]\n",
        "\n",
        "            # Column 1: NAIP RGB\n",
        "            ax0.imshow(rgb_img)\n",
        "            ax0.set_title(f\"NAIP RGB (tile {idx})\")\n",
        "            ax0.axis(\"off\")\n",
        "\n",
        "            # Column 2: Ground truth label\n",
        "            ax1.imshow(mask_np, cmap=label_cmap, norm=label_norm)\n",
        "            ax1.set_title(\"Label (0=bg, 1=TOP, 2=TNP)\")\n",
        "            ax1.axis(\"off\")\n",
        "\n",
        "            # Column 3: Student prediction\n",
        "            ax2.imshow(pred_student, cmap=label_cmap, norm=label_norm)\n",
        "            ax2.set_title(\"Student UNet (NAIP+KD)\")\n",
        "            ax2.axis(\"off\")\n",
        "\n",
        "            # Column 4: Baseline prediction\n",
        "            ax3.imshow(pred_baseline, cmap=label_cmap, norm=label_norm)\n",
        "            ax3.set_title(\"Baseline UNet (NAIP-only)\")\n",
        "            ax3.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(title, y=0.99, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Call it on your test set:\n",
        "visualize_student_baseline(\n",
        "    student_unet_naip,\n",
        "    test_ds,\n",
        "    n_samples=5,\n",
        "    rgb_indices=(0,1,2),\n",
        "    naip_channels=NAIP_CHANNELS,\n",
        "    title=\"Student vs Baseline – Test tiles\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "3GTNConNXIL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
